{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "from sklearn.metrics import recall_score\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "We have a database (ULL_database) with information about primary and secondary education students in the Canary Islands \n",
    "for 4 academic years. There is information about their academic performance and \n",
    "contextual information (about their families, teachers, and school). The database contains a subset of data \n",
    "in the form of panel data, meaning information about the same students at different points in time (ULL_panel_data).\n",
    "\n",
    "Machine learning algorithms can be used to predict at-risk students. \n",
    "A student is considered at risk if they are anticipated to have low academic performance in the future. \n",
    "Detecting these students would allow for corrective measures to be taken in advance.\n",
    "\n",
    "As a measure of academic performance, we have the variables \"scores\".\n",
    "We have academic performance in Mathematics and in Spanish Language\n",
    "\n",
    "We specify a model to predict at-risk students. Utilizing the panel data,\n",
    "the model aims to forecast whether the student will be at risk in the future (in 6th grade)\n",
    "based on various predictors of current academic performance (3rd grade).\n",
    "\n",
    "Each observation (row) in ULL_panel_data is a student, with their academic performance in sixth grade \n",
    "and their predictors of academic performance from third grade (columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'data/'\n",
    "data = pd.read_csv(os.path.join(DATA, 'ULL_panel_data.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the data we want to work for\n",
    "data = data[['id_student_16_19', 'score_MAT', 'score_LEN', 'score_MAT3', 'score_LEN3', 'a1',\n",
    "             'mother_education', 'father_education', 'mother_occupation', 'father_occupation', \n",
    "             'inmigrant_second_gen', 'start_schooling_age', 'books', 'f12a', 'public_private', \n",
    "             'capital_island', 'd14', 'ESCS', 'id_school']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations with missing data in any of the variables that we will use in the models\n",
    "# Here, synthetic data methods can be used instead to fill in missing values\n",
    "\n",
    "missing_columns = ['score_MAT3', 'a1', 'mother_education', 'father_education',\n",
    "    'mother_occupation', 'father_occupation', 'inmigrant_second_gen',\n",
    "    'start_schooling_age', 'books', 'f12a', 'public_private',\n",
    "    'capital_island', 'd14']\n",
    "\n",
    "data = data.dropna(subset=missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns \n",
    "data = pd.DataFrame(data.values.flatten().reshape(-1, data.shape[1]), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate quartiles of scores in sixth grade\n",
    "data['scores_MATq'] = pd.qcut(data['score_MAT'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_MATq'] = data['scores_MATq'].astype(int)\n",
    "data['scores_LENq'] = pd.qcut(data['score_LEN'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_LENq'] = data['scores_LENq'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate median and percentiles 25 and 75 of socioeconomic status (ESCS)\n",
    "median_ESCS = data['ESCS'].median()\n",
    "p25_ESCS = data['ESCS'].quantile(0.25)\n",
    "p75_ESCS = data['ESCS'].quantile(0.75)\n",
    "\n",
    "# Initialize with null values\n",
    "data['ESCS_median'] = pd.Series([np.nan] * len(data))\n",
    "data.loc[data['ESCS'] >= median_ESCS, 'ESCS_median'] = 2\n",
    "data.loc[data['ESCS'] < median_ESCS, 'ESCS_median'] = 1\n",
    "data.loc[data['ESCS_median'] == 0, 'ESCS_median'] = np.nan\n",
    "\n",
    "# Initialize with null values\n",
    "data['ESCS_p25_p75'] = pd.Series([np.nan] * len(data))\n",
    "data.loc[data['ESCS'] >= p75_ESCS, 'ESCS_p25_p75'] = 2\n",
    "data.loc[data['ESCS'] < p25_ESCS, 'ESCS_p25_p75'] = 1\n",
    "data.loc[(data['ESCS'] >= p25_ESCS) & (data['ESCS'] < p75_ESCS), 'ESCS_p25_p75'] = np.nan\n",
    "\n",
    "# Some data corrections to make the final results\n",
    "# Variable d14 top category(4) is the \"bad\" category (more than 50% of teachers change school), so the results must be inverted\n",
    "# isn't this applying the identity?\n",
    "data['d14'] = data['d14'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The goal of the model is to predict the academic performance in sixth grade ($Y_t$)\n",
    "using information from the same student in third grade, specifically:\n",
    "\n",
    "1.  Academic performance in third grade ($Y_{t-1}$)\n",
    "\n",
    "2.  Sensitive factors or circumstances ($C$)\n",
    "\n",
    "3.  Predictors uncorrelated with circumstances, also called \"effort\" ($X$)\n",
    "\n",
    "**Model 1**:    $$Y_t = α + β1Y_{t-1} + ε$$\n",
    "\n",
    "**Model 2**:    $$Y_t = α + β1Y_{t-1} + β2C + ε$$\n",
    "\n",
    "**Model 3**:    \n",
    "\n",
    "> First step: $$Y_{t-1} = α + β2C + ν$$\n",
    "\n",
    "- Recover the prediction of $Y_{t-1}$ (academic performance due to circumstances, $C$): $\\hat{Y}_{t-1}$\n",
    "\n",
    "- Recover the residual $ν$ (academic performance due to effort, $X$): $\\hat{ν}$\n",
    "\n",
    "> Second step: $$Y_t = α + β1\\hat{Y}_{t-1} + β2\\hat{ν} + ε$$\n",
    "\n",
    "- Recover the prediction of $Y_t$ only due to $\\hat{Y}_{t-1}$ (only due to circumstances)\n",
    "\n",
    "- Recover the prediction of $Y_t$ only due to $\\hat{ν}$ (only due to effort)\n",
    "\n",
    "In theory...\n",
    "\n",
    "**Model 1**: Using only the academic performance in third grade (benchmark)\n",
    "\n",
    "**Model 2**: Using the academic performance + circumstances in third grade (less fair - more socially desirable)\n",
    "\n",
    "**Model 3**: Using the circumstances + effort in third grade (close to Model 2)\n",
    "\n",
    "- Prediction exclusively of circumstances of Model 3 (much less fair - much more socially desirable)\n",
    "    \n",
    "- Prediction exclusively of effort of Model 3 (much more fair - much less socially desirable)\n",
    "\n",
    "Let's prove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for the models\n",
    "Y_t_1 = \"score_MAT3\"\n",
    "c = data[[\"a1\", \"mother_education\", \"father_education\", \"mother_occupation\", \"father_occupation\", \n",
    "      \"inmigrant_second_gen\", \"start_schooling_age\", \"books\", \"f12a\", \"public_private\", \"capital_island\", \"d14\"]]\n",
    "circumstances = [\"a1\", \"mother_education\", \"father_education\", \"mother_occupation\", \"father_occupation\", \n",
    "      \"inmigrant_second_gen\", \"start_schooling_age\", \"books\", \"f12a\", \"public_private\", \"capital_island\", \"d14\"]\n",
    "\n",
    "# Dummy variables (all variables C are categorical variables)\n",
    "dummy_variables = pd.get_dummies(c, columns=circumstances, drop_first = True)\n",
    "\n",
    "# Join Y_t_1 + C\n",
    "data_combined = pd.concat([data[Y_t_1], dummy_variables], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "model1 = sm.OLS(data[\"score_MAT\"], sm.add_constant(data[Y_t_1])).fit()\n",
    "print(model1.summary())\n",
    "data['model1_pred'] = model1.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "model2 = sm.OLS(data[\"score_MAT\"], sm.add_constant(data_combined.astype(np.float64))).fit()\n",
    "print(model2.summary())\n",
    "data['model2_pred'] = model2.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "model3 = sm.OLS(data[\"score_MAT3\"], sm.add_constant(dummy_variables.astype(np.float64))).fit()\n",
    "print(model3.summary())\n",
    "\n",
    "# First step\n",
    "data['Y_t_1_hat'] = model3.fittedvalues\n",
    "data['ν_hat'] = model3.resid\n",
    "\n",
    "# Second step\n",
    "model4 = sm.OLS(data[\"score_MAT\"], sm.add_constant(data[[\"Y_t_1_hat\", \"ν_hat\"]])).fit()\n",
    "print(model4.summary())\n",
    "data['model3_pred'] = model3.fittedvalues\n",
    "\n",
    "# Prediction exclusively of circumstances\n",
    "data['model3_pred_circum'] = model4.params['const'] + model4.params['Y_t_1_hat'] * data['Y_t_1_hat']\n",
    "# Prediction exclusively of effort\n",
    "mean_circu = data['Y_t_1_hat'].mean()\n",
    "data['mean_circu'] = mean_circu\n",
    "data['model3_pred_effort'] = (model4.params['const'] + \n",
    "                          model4.params['ν_hat'] * data['ν_hat'] + \n",
    "                          model4.params['Y_t_1_hat'] * mean_circu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform predictions(continuous) to quartiles(categorical)\n",
    "\n",
    "data['scores_MAT_pred1'] = pd.qcut(data['model1_pred'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_MAT_pred1'] = data['scores_MAT_pred1'].astype(int)\n",
    "data['scores_MAT_pred2'] = pd.qcut(data['model2_pred'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_MAT_pred2'] = data['scores_MAT_pred2'].astype(int)\n",
    "data['scores_MAT_pred3'] = pd.qcut(data['model3_pred'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_MAT_pred3'] = data['scores_MAT_pred3'].astype(int)\n",
    "data['scores_MAT_pred_C'] = pd.qcut(data['model3_pred_circum'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_MAT_pred_C'] = data['scores_MAT_pred_C'].astype(int)\n",
    "data['scores_MAT_pred_X'] = pd.qcut(data['model3_pred_effort'], 4, labels=[\"1\", \"2\", \"3\",\"4\"])\n",
    "data['scores_MAT_pred_X'] = data['scores_MAT_pred_X'].astype(int)\n",
    "\n",
    "# Transform predictions(continuous) to percentiles but percentiles 2 and 3 equal (between 25th and 75th percentile)\n",
    "\n",
    "data['scores_MAT_pred1_t'] = data['scores_MAT_pred1'].apply(lambda x: 1 if x == 1 else (2 if x == 2 or x == 3 else 3))\n",
    "data['scores_MAT_pred2_t'] = data['scores_MAT_pred2'].apply(lambda x: 1 if x == 1 else (2 if x == 2 or x == 3 else 3))\n",
    "data['scores_MAT_pred3_t'] = data['scores_MAT_pred3'].apply(lambda x: 1 if x == 1 else (2 if x == 2 or x == 3 else 3))\n",
    "data['scores_MAT_pred_C_t'] = data['scores_MAT_pred_C'].apply(lambda x: 1 if x == 1 else (2 if x == 2 or x == 3 else 3))\n",
    "data['scores_MAT_pred_X_t'] = data['scores_MAT_pred_X'].apply(lambda x: 1 if x == 1 else (2 if x == 2 or x == 3 else 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on **Equalized Odds** (Equality of opportunity).\n",
    "\n",
    "To calculate Equalized Odds we first calculate recall or sensitivity:\n",
    "\n",
    "$$TP / (TP + FN)$$\n",
    "\n",
    "and then we calculate the ratio of recall among different groups to obtain Equalized Odds.\n",
    "\n",
    "Recall is calculated for Low and High academic performance:\n",
    "- **Low academic performance**: Below the median or 25th percentile\n",
    "- **High academic performance**: Above the median or above 75th percentile (top 25 percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_dfs_25_75 = []\n",
    "recall_dfs_25_75.extend(compute_recall(data, [\"f12a\"], top_level=5))\n",
    "recall_dfs_25_75.extend(compute_recall(data, [\"mother_education\", \"father_education\", \"mother_occupation\", \"father_occupation\", \"books\"], top_level=4))\n",
    "recall_dfs_25_75.extend(compute_recall(data, [\"start_schooling_age\"], top_level=1))\n",
    "recall_dfs_25_75.extend(compute_recall(data, [\"inmigrant_second_gen\", \"public_private\", \"capital_island\", \"a1\", \"ESCS_median\", \"ESCS_p25_p75\", \"d14\"], top_level=1))\n",
    "\n",
    "recall_dfs_between25_75 = []\n",
    "recall_dfs_between25_75.extend(compute_recall_terciles(data, [\"f12a\"], top_level=5))\n",
    "recall_dfs_between25_75.extend(compute_recall_terciles(data, [\"mother_education\", \"father_education\", \"mother_occupation\", \"father_occupation\", \"books\"], top_level=4))\n",
    "recall_dfs_between25_75.extend(compute_recall_terciles(data, [\"start_schooling_age\"], top_level=1))\n",
    "recall_dfs_between25_75.extend(compute_recall_terciles(data, [\"inmigrant_second_gen\", \"public_private\", \"capital_island\", \"a1\", \"ESCS_median\", \"ESCS_p25_p75\", \"d14\"], top_level=1))\n",
    "\n",
    "recall_dfs_median = []\n",
    "recall_dfs_median.extend(compute_recall_median(data, [\"f12a\"], top_level=5))\n",
    "recall_dfs_median.extend(compute_recall_median(data, [\"mother_education\", \"father_education\", \"mother_occupation\", \"father_occupation\", \"books\"], top_level=4))\n",
    "recall_dfs_median.extend(compute_recall_median(data, [\"start_schooling_age\"], top_level=1))\n",
    "recall_dfs_median.extend(compute_recall_median(data, [\"inmigrant_second_gen\", \"public_private\", \"capital_island\", \"a1\", \"ESCS_median\", \"ESCS_p25_p75\", \"d14\"], top_level=1))\n",
    "\n",
    "# Combine DataFrames\n",
    "combined_df_25_75 = pd.concat(recall_dfs_25_75, ignore_index=True)\n",
    "combined_df_between25_75 = pd.concat(recall_dfs_between25_75, ignore_index=True)\n",
    "combined_df_median = pd.concat(recall_dfs_median, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables\n",
    "pivot_combined_df_25_75 = combined_df_25_75.pivot_table(index=['Variable', 'Group', 'Percentile'], columns='Model', values='Recall').reset_index()\n",
    "pivot_combined_df_25_75 = pivot_combined_df_25_75[['Variable', 'Group', 'Percentile', 'pred1', 'pred2', 'pred3', 'pred_C', 'pred_X']]\n",
    "pivot_combined_df_25_75_sorted = pivot_combined_df_25_75.sort_values(by=['Percentile', 'Variable', 'Group'], ascending=[True, True, False])\n",
    "pivot_combined_df_between25_75 = combined_df_between25_75.pivot_table(index=['Variable', 'Group', 'Tercile'], columns='Model', values='Recall').reset_index()\n",
    "pivot_combined_df_between25_75 = pivot_combined_df_between25_75[['Variable', 'Group', 'Tercile', 'pred1_t', 'pred2_t', 'pred3_t', 'pred_C_t', 'pred_X_t']]\n",
    "pivot_combined_df_between25_75_sorted = pivot_combined_df_between25_75.sort_values(by=['Tercile', 'Variable', 'Group'], ascending=[True, True, False])\n",
    "pivot_combined_df_median = combined_df_median.pivot_table(index=['Variable', 'Group', 'Pair1', 'Pair2'], columns='Model', values='Recall').reset_index()\n",
    "pivot_combined_df_median = pivot_combined_df_median[['Variable', 'Group', 'Pair1', 'Pair2', 'pred1', 'pred2', 'pred3', 'pred_C', 'pred_X']]\n",
    "pivot_combined_df_median_sorted = pivot_combined_df_median.sort_values(by=['Pair1', 'Pair2', 'Variable', 'Group'], ascending=[True, True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_25_75 = []\n",
    "\n",
    "for variable in pivot_combined_df_25_75_sorted['Variable'].unique():\n",
    "    variable_df = pivot_combined_df_25_75_sorted[pivot_combined_df_25_75_sorted['Variable'] == variable]\n",
    "    for percentile in variable_df['Percentile'].unique():\n",
    "        top_row = variable_df[(variable_df['Group'] == 'top') & (variable_df['Percentile'] == percentile)]\n",
    "        if not top_row.empty:\n",
    "            top_row = top_row.iloc[0]\n",
    "            temp_data = []\n",
    "            for _, row in variable_df[variable_df['Percentile'] == percentile].iterrows():\n",
    "                odds_row = {\n",
    "                    'Variable': row['Variable'],\n",
    "                    'Group': row['Group'],\n",
    "                    'Percentile': row['Percentile'],\n",
    "                    'pred1': row['pred1'],\n",
    "                    'pred2': row['pred2'],\n",
    "                    'pred3': row['pred3'],\n",
    "                    'pred_C': row['pred_C'],\n",
    "                    'pred_X': row['pred_X'],\n",
    "                    'pred1_odds': calculate_odds(row['pred1'], top_row['pred1']),\n",
    "                    'pred2_odds': calculate_odds(row['pred2'], top_row['pred2']),\n",
    "                    'pred3_odds': calculate_odds(row['pred3'], top_row['pred3']),\n",
    "                    'pred_C_odds': calculate_odds(row['pred_C'], top_row['pred_C']),\n",
    "                    'pred_X_odds': calculate_odds(row['pred_X'], top_row['pred_X']),\n",
    "                }\n",
    "                temp_data.append(odds_row)\n",
    "            final_data_25_75.extend(temp_data)\n",
    "\n",
    "final_data_25_75_sorted = pd.DataFrame(final_data_25_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_between25_75 = []\n",
    "\n",
    "for variable in pivot_combined_df_between25_75_sorted['Variable'].unique():\n",
    "    variable_df = pivot_combined_df_between25_75_sorted[pivot_combined_df_between25_75_sorted['Variable'] == variable]\n",
    "    for tercile in variable_df['Tercile'].unique():\n",
    "        top_row = variable_df[(variable_df['Group'] == 'top') & (variable_df['Tercile'] == tercile)]\n",
    "        if not top_row.empty:\n",
    "            top_row = top_row.iloc[0]\n",
    "            temp_data = []\n",
    "            for _, row in variable_df[variable_df['Tercile'] == tercile].iterrows():\n",
    "                odds_row = {\n",
    "                    'Variable': row['Variable'],\n",
    "                    'Group': row['Group'],\n",
    "                    'Tercile': row['Tercile'],\n",
    "                    'pred1_t': row['pred1_t'],\n",
    "                    'pred2_t': row['pred2_t'],\n",
    "                    'pred3_t': row['pred3_t'],\n",
    "                    'pred_C_t': row['pred_C_t'],\n",
    "                    'pred_X_t': row['pred_X_t'],\n",
    "                    'pred1_odds': calculate_odds(row['pred1_t'], top_row['pred1_t']),\n",
    "                    'pred2_odds': calculate_odds(row['pred2_t'], top_row['pred2_t']),\n",
    "                    'pred3_odds': calculate_odds(row['pred3_t'], top_row['pred3_t']),\n",
    "                    'pred_C_odds': calculate_odds(row['pred_C_t'], top_row['pred_C_t']),\n",
    "                    'pred_X_odds': calculate_odds(row['pred_X_t'], top_row['pred_X_t']),\n",
    "                }\n",
    "                temp_data.append(odds_row)\n",
    "            final_data_between25_75.extend(temp_data)\n",
    "\n",
    "final_data_between25_75_sorted = pd.DataFrame(final_data_between25_75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_median = []\n",
    "\n",
    "for variable in pivot_combined_df_median_sorted['Variable'].unique():\n",
    "    variable_df = pivot_combined_df_median_sorted[pivot_combined_df_median_sorted['Variable'] == variable]\n",
    "    for pair in variable_df[['Pair1', 'Pair2']].drop_duplicates().values:\n",
    "        pair1, pair2 = pair\n",
    "        top_row = variable_df[(variable_df['Group'] == 'top') & (variable_df['Pair1'] == pair1) & (variable_df['Pair2'] == pair2)]\n",
    "        if not top_row.empty:\n",
    "            top_row = top_row.iloc[0]\n",
    "            temp_data = []\n",
    "            for _, row in variable_df[(variable_df['Pair1'] == pair1) & (variable_df['Pair2'] == pair2)].iterrows():\n",
    "                odds_row = {\n",
    "                    'Variable': row['Variable'],\n",
    "                    'Group': row['Group'],\n",
    "                    'Pair1': row['Pair1'],\n",
    "                    'Pair2': row['Pair2'],\n",
    "                    'pred1': row['pred1'],\n",
    "                    'pred2': row['pred2'],\n",
    "                    'pred3': row['pred3'],\n",
    "                    'pred_C': row['pred_C'],\n",
    "                    'pred_X': row['pred_X'],\n",
    "                    'pred1_odds': calculate_odds(row['pred1'], top_row['pred1']),\n",
    "                    'pred2_odds': calculate_odds(row['pred2'], top_row['pred2']),\n",
    "                    'pred3_odds': calculate_odds(row['pred3'], top_row['pred3']),\n",
    "                    'pred_C_odds': calculate_odds(row['pred_C'], top_row['pred_C']),\n",
    "                    'pred_X_odds': calculate_odds(row['pred_X'], top_row['pred_X']),\n",
    "                }\n",
    "                temp_data.append(odds_row)\n",
    "            final_data_median.extend(temp_data)\n",
    "\n",
    "final_data_median_sorted = pd.DataFrame(final_data_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_order = ['a1', 'mother_education', 'father_education', 'mother_occupation', 'father_occupation', 'books', 'd14', 'inmigrant_second_gen', \n",
    "                  'public_private', 'capital_island', 'start_schooling_age', 'f12a', 'ESCS_median', 'ESCS_p25_p75']\n",
    "\n",
    "final_data_25_75_sorted['Variable'] = pd.Categorical(final_data_25_75_sorted['Variable'], categories=category_order, ordered=True)\n",
    "final_data_25_75_sorted = final_data_25_75_sorted.sort_values(by='Variable')\n",
    "final_data_25_75_sorted = final_data_25_75_sorted[['Variable', 'Group', 'Percentile', 'pred1', 'pred1_odds', 'pred2', 'pred2_odds', 'pred3', 'pred3_odds', 'pred_C', 'pred_C_odds', 'pred_X', 'pred_X_odds']]\n",
    "final_data_25_75_sorted = final_data_25_75_sorted.sort_values(by=['Percentile', 'Variable', 'Group'], ascending=[True, True, False])\n",
    "final_data_between25_75_sorted['Variable'] = pd.Categorical(final_data_between25_75_sorted['Variable'], categories=category_order, ordered=True)\n",
    "final_data_between25_75_sorted = final_data_between25_75_sorted.sort_values(by='Variable')\n",
    "final_data_between25_75_sorted = final_data_between25_75_sorted[['Variable', 'Group', 'Tercile', 'pred1_t', 'pred1_odds', 'pred2_t', 'pred2_odds', 'pred3_t', 'pred3_odds', 'pred_C_t', 'pred_C_odds', 'pred_X_t', 'pred_X_odds']]\n",
    "final_data_between25_75_sorted = final_data_between25_75_sorted.sort_values(by=['Tercile', 'Variable', 'Group'], ascending=[True, True, False])\n",
    "final_data_median_sorted['Variable'] = pd.Categorical(final_data_median_sorted['Variable'], categories=category_order, ordered=True)\n",
    "final_data_median_sorted = final_data_median_sorted.sort_values(by='Variable')\n",
    "final_data_median_sorted = final_data_median_sorted[['Variable', 'Group', 'Pair1', 'Pair2', 'pred1', 'pred1_odds', 'pred2', 'pred2_odds', 'pred3', 'pred3_odds', 'pred_C', 'pred_C_odds', 'pred_X', 'pred_X_odds']]\n",
    "final_data_median_sorted = final_data_median_sorted.sort_values(by=['Pair1', 'Pair2', 'Variable', 'Group'], ascending=[True, True, True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel\n",
    "with pd.ExcelWriter(os.path.join('results', 'results.xlsx')) as writer:\n",
    "    final_data_25_75_sorted.to_excel(writer, sheet_name='25_75', index=False, float_format='%.4f')\n",
    "    final_data_median_sorted.to_excel(writer, sheet_name='Median', index=False, float_format='%.4f')\n",
    "    final_data_between25_75_sorted.to_excel(writer, sheet_name='between25_75', index=False, float_format='%.4f')\n",
    "    data.to_excel(writer, sheet_name='data', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOP\n",
    "\n",
    "Inequality of Opportunity is computed by applying an inequality index (Gini, MLD or simple variance) to the set of central moments (specifically, the mean $\\mu$) for the $Y$'s conditional distributions with respect to a sensitive attribute's values. Mathematically:\n",
    "$$IOP = I(\\mu(Y|G_{i}), \\ldots, \\mu(Y|G_{m}))$$ \n",
    "\n",
    "$I$ is the inequality index while $G_{1} \\ldots G_{m}$ are the $m$ different groups of individuals identified by the values a given senstive attribute can have. For example, if _gender_ is a sensitive attribute then the two resulting groups might be $G_{1} = male$ and $G_{2} = female$. In this situation, _IOP_ would be absent if $$I(\\mu(Y|G_{1}),\\mu(Y|G_{2})) = 0$$ or close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_attrs = ['a1', 'mother_education', 'father_education',\n",
    "       'mother_occupation', 'father_occupation', 'inmigrant_second_gen',\n",
    "       'start_schooling_age', 'books', 'f12a', 'public_private',\n",
    "       'capital_island', 'd14', \"ESCS_median\", \"ESCS_p25_p75\"]\n",
    "\n",
    "preds = [\"model1_pred\", \"model2_pred\", \"model3_pred\", \"model3_pred_circum\", \"model3_pred_effort\"]\n",
    "\n",
    "model_pred_rename = {\n",
    "    \"model1_pred\": \"Model 1\",\n",
    "    \"model2_pred\": \"Model 2\",\n",
    "    \"model3_pred\": \"Model 3\",\n",
    "    \"model3_pred_circum\": \"Circumstances\",\n",
    "    \"model3_pred_effort\": \"Effort\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below 25th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = {}\n",
    "\n",
    "for pred in preds:\n",
    "    col = []\n",
    "    for sa in sensitive_attrs:\n",
    "        data[\"label\"] = binarise_predictions(data[pred], \"below-25\")\n",
    "        col.append(iop(data, sa))\n",
    "    df_res[model_pred_rename[pred]] = col\n",
    "\n",
    "df_res = pd.DataFrame(df_res, index=sensitive_attrs)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between 25th and 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = {}\n",
    "\n",
    "for pred in preds:\n",
    "    col = []\n",
    "    for sa in sensitive_attrs:\n",
    "        data[\"label\"] = binarise_predictions(data[pred], \"between-25-75\")\n",
    "        col.append(iop(data, sa))\n",
    "    df_res[model_pred_rename[pred]] = col\n",
    "\n",
    "df_res = pd.DataFrame(df_res, index=sensitive_attrs)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = {}\n",
    "\n",
    "for pred in preds:\n",
    "    col = []\n",
    "    for sa in sensitive_attrs:\n",
    "        data[\"label\"] = binarise_predictions(data[pred], \"above-75\")\n",
    "        col.append(iop(data, sa))\n",
    "    df_res[model_pred_rename[pred]] = col\n",
    "\n",
    "df_res = pd.DataFrame(df_res, index=sensitive_attrs)\n",
    "df_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
